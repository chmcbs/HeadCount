{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c621b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5f4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8c25b",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854f7f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "✓ Loaded 1096 examples from disk\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "dataset_path = Path(\"./data\")\n",
    "\n",
    "# Check if dataset is saved to disk\n",
    "if dataset_path.exists() and (dataset_path / \"dataset_dict.json\").exists():\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    dataset = load_from_disk(str(dataset_path))\n",
    "    print(f\"✓ Loaded {len(dataset['labelled'])} examples from disk\")\n",
    "    \n",
    "# If not on disk, check if dataset variable exists in memory\n",
    "elif 'dataset' in globals() and dataset is not None:\n",
    "    print(\"Dataset variable found in memory, saving to disk...\")\n",
    "    dataset.save_to_disk(str(dataset_path))\n",
    "    print(f\"✓ Saved {len(dataset['labelled'])} examples to disk\")\n",
    "    \n",
    "# Otherwise, load from HuggingFace and save\n",
    "else:\n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"GlobalWheat/GWFSS_v1.0\")\n",
    "    print(f\"✓ Loaded {len(dataset['labelled'])} examples from HuggingFace\")\n",
    "    \n",
    "    print(\"Saving dataset to disk...\")\n",
    "    dataset.save_to_disk(str(dataset_path))\n",
    "    print(f\"✓ Saved dataset to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af55ca",
   "metadata": {},
   "source": [
    "### Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c886cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded class counts from cache\n",
      "✓ Created stratification labels\n"
     ]
    }
   ],
   "source": [
    "# Define class mapping and class counts path\n",
    "class_counts_path = Path(\"./cache/class_counts.json\")\n",
    "rgb_to_class = {\n",
    "    (0, 0, 0): 0,              # Background (Black)\n",
    "    (214, 255, 50): 1,         # Leaf (Yellow-green)\n",
    "    (50, 132, 255): 2,         # Stem (Blue)\n",
    "    (50, 255, 132): 3,         # Head (Cyan-green)\n",
    "}\n",
    "\n",
    "# Load class counts if available\n",
    "if class_counts_path.exists():\n",
    "    with open(class_counts_path, 'r') as f:\n",
    "        class_counts_dict = json.load(f)\n",
    "    class_counts = defaultdict(list, {int(k): v for k, v in class_counts_dict.items()})\n",
    "    print(f\"✓ Loaded class counts from cache\")\n",
    "else:\n",
    "    print(\"⚠ Class counts not found. Please run the EDA notebook first.\")\n",
    "\n",
    "# Define function to get the dominant class of an image\n",
    "def get_dominant_class(image_idx):\n",
    "    counts = [class_counts[class_id][image_idx] for class_id in range(4)]\n",
    "    return np.argmax(counts)\n",
    "\n",
    "# Create a list containing the dominant class label for each image (0-3)\n",
    "stratify_labels = [get_dominant_class(i) for i in range(len(dataset[\"labelled\"]))]\n",
    "print(f\"✓ Created stratification labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e52448",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e54be",
   "metadata": {},
   "source": [
    "To split the data into three groups (Train/Val/Test), we need to do two separate splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa958750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created stratified splits:\n",
      "  Train: 767 examples (70.0%)\n",
      "  Val:   164 examples (15.0%)\n",
      "  Test:  165 examples (15.1%)\n"
     ]
    }
   ],
   "source": [
    "# Split 1: Train/ValTest (70:30)\n",
    "train_indices, valtest_indices = train_test_split(\n",
    "    range(len(dataset[\"labelled\"])),\n",
    "    test_size=0.3,\n",
    "    stratify=stratify_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split 2: Val/Test (50:50)\n",
    "valtest_labels = [stratify_labels[i] for i in valtest_indices]\n",
    "val_indices, test_indices = train_test_split(\n",
    "    valtest_indices,\n",
    "    test_size=0.5,  # 50% of ValTest = 15% of total\n",
    "    stratify=valtest_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create splits\n",
    "train_split = dataset[\"labelled\"].select(train_indices)\n",
    "val_split = dataset[\"labelled\"].select(val_indices)\n",
    "test_split = dataset[\"labelled\"].select(test_indices)\n",
    "\n",
    "print(f\"✓ Created stratified splits:\")\n",
    "print(f\"  Train: {len(train_split)} examples ({100*len(train_split)/len(dataset['labelled']):.1f}%)\")\n",
    "print(f\"  Val:   {len(val_split)} examples ({100*len(val_split)/len(dataset['labelled']):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_split)} examples ({100*len(test_split)/len(dataset['labelled']):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4300954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset dominant class distribution:\n",
      "  Background: 301 (27.5%)\n",
      "  Leaf: 772 (70.4%)\n",
      "  Stem: 0 (0.0%)\n",
      "  Head: 23 (2.1%)\n",
      "\n",
      "Train dominant class distribution:\n",
      "  Background: 211 (27.5%)\n",
      "  Leaf: 540 (70.4%)\n",
      "  Stem: 0 (0.0%)\n",
      "  Head: 16 (2.1%)\n",
      "\n",
      "Val dominant class distribution:\n",
      "  Background: 45 (27.4%)\n",
      "  Leaf: 116 (70.7%)\n",
      "  Stem: 0 (0.0%)\n",
      "  Head: 3 (1.8%)\n",
      "\n",
      "Test dominant class distribution:\n",
      "  Background: 45 (27.3%)\n",
      "  Leaf: 116 (70.3%)\n",
      "  Stem: 0 (0.0%)\n",
      "  Head: 4 (2.4%)\n"
     ]
    }
   ],
   "source": [
    "# Define function to check dominant class distribution in a split\n",
    "def check_stratification(split_name, split_indices):\n",
    "    split_labels = [stratify_labels[i] for i in split_indices]\n",
    "    class_names = ['Background', 'Leaf', 'Stem', 'Head']\n",
    "    print(f\"\\n{split_name} dominant class distribution:\")\n",
    "    for class_id, class_name in enumerate(class_names):\n",
    "        count = sum(1 for label in split_labels if label == class_id)\n",
    "        pct = 100 * count / len(split_labels)\n",
    "        print(f\"  {class_name}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Check the dominant class distribution of the full dataset        \n",
    "check_stratification(\"Full Dataset\", range(len(dataset[\"labelled\"])))\n",
    "\n",
    "# Check the dominant class distribution of each split\n",
    "check_stratification(\"Train\", train_indices)\n",
    "check_stratification(\"Val\", val_indices)\n",
    "check_stratification(\"Test\", test_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
